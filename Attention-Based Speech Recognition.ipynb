{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8a_cZqr-UpV"
      },
      "source": [
        "# HW4P2: Attention-based Speech Recognition\n",
        "\n",
        "Welcome to the final assignment in 11785. In this HW, you will work on building a speech recognition system with attention. <br> <br>\n",
        "\n",
        "HW Writeup: https://piazza.com/class_profile/get_resource/l37uyxe87cq5xn/lam1lcjjj0314e <br>\n",
        "Kaggle competition link: https://www.kaggle.com/competitions/11-785-f22-hw4p2/ <br>\n",
        "LAS Paper: https://arxiv.org/pdf/1508.01211.pdf <br>\n",
        "Attention is all you need:https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlev_Tvq_bRz"
      },
      "source": [
        "# Initial Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pueIzbxUwyY",
        "outputId": "63a2330b-5c86-4dee-a1f8-89da67f76347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Dec 11 22:06:45 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P0    27W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tepaid_TDwWt",
        "outputId": "c9f309cd-edda-4dc9-9090-eb8903293d98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9 MB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 174 kB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 59.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 173 kB 57.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 59.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 47.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 61.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 52.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 68.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 75.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 38.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 62.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 62.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 58.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 61.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 58.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 59.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 56.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 52.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 67.2 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install some required libraries\n",
        "# Feel free to add more if you want\n",
        "!pip install -q python-levenshtein torchsummaryX wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "olsi9P01rsaa",
        "outputId": "4b6f0c02-a407-46c1-9239-bc77b06f6ab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-nlp\n",
            "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▋                            | 10 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 20 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 40 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 51 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 61 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 71 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 81 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 90 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch-nlp) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch-nlp) (1.21.6)\n",
            "Installing collected packages: pytorch-nlp\n",
            "Successfully installed pytorch-nlp-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr4xGzRU-KZz"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjiKG0pOCscq",
        "outputId": "66bbbaa1-558a-4b1a-db61-d3f02eeb3057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZectxKF3XEVV",
        "outputId": "e6439578-b179-42f7-a72a-93ece95eba1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import Levenshtein\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torchnlp.nn import LockedDropout\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import gc\n",
        "from torchsummaryX import summary\n",
        "import wandb\n",
        "from glob import glob\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K5HLepZpA5WS"
      },
      "outputs": [],
      "source": [
        "# # TODO: Import drive if you are a colab user\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OALQCI0EDCwh"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqDuibMCP345"
      },
      "outputs": [],
      "source": [
        "# Global config dict. Feel free to add or change if you want.\n",
        "config = {\n",
        "    'batch_size': 96,\n",
        "    'epochs': 60,\n",
        "    'lr': 1e-3\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7J4sY1OW9Pr"
      },
      "source": [
        "# Toy Data Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTsQB-pvRLLs"
      },
      "source": [
        "The toy dataset is very essential for you in this HW. The model which you will be building is complicated and you first need to make sure that it runs on the toy dataset. <br>\n",
        "In other words, you need convergence - the attention diagonal. Take a look at the write-up for this. <br>\n",
        "We have given you the following code to download the toy data and load it. You can use it the way it is. But be careful, the transcripts are different from the original data from kaggle. The toy dataset has phonemes but the actual data has characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Des4AMIaW4E8",
        "outputId": "a2ce5674-b809-4d22-9243-315875566326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f0176_mfccs_train.n 100%[===================>] 279.30M  12.5MB/s    in 22s     \n",
            "f0176_mfccs_dev.npy 100%[===================>]  27.93M  13.3MB/s    in 2.1s    \n",
            "f0176_hw3p2_train.n 100%[===================>]   7.02M  5.21MB/s    in 1.3s    \n",
            "f0176_hw3p2_dev.npy 100%[===================>] 718.88K   905KB/s    in 0.8s    \n"
          ]
        }
      ],
      "source": [
        "!wget -q https://cmu.box.com/shared/static/wok08c2z2dp4clufhy79c5ee6jx3pyj9 --content-disposition --show-progress\n",
        "!wget -q https://cmu.box.com/shared/static/zctr6mvh7npfn01forli8n45duhp2g85 --content-disposition --show-progress\n",
        "!wget -q https://cmu.box.com/shared/static/m2oaek69145ljeu6srtbbb7k0ip6yfup --content-disposition --show-progress\n",
        "!wget -q https://cmu.box.com/shared/static/owrjy0tqra3v7zq2ru7mocy2djskydy9 --content-disposition --show-progress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fLveDeiXCsb"
      },
      "outputs": [],
      "source": [
        "# Load the toy dataset\n",
        "X_train = np.load(\"f0176_mfccs_train.npy\")\n",
        "X_valid = np.load(\"f0176_mfccs_dev.npy\")\n",
        "Y_train = np.load(\"f0176_hw3p2_train.npy\")\n",
        "Y_valid = np.load(\"f0176_hw3p2_dev.npy\")\n",
        "\n",
        "# This is how you actually need to find out the different trancripts in a dataset. \n",
        "# Can you think whats going on here? Why are we using a np.unique?\n",
        "VOCAB_MAP           = dict(zip(np.unique(Y_valid), range(len(np.unique(Y_valid))))) \n",
        "VOCAB_MAP[\"[PAD]\"]  = len(VOCAB_MAP)\n",
        "VOCAB               = list(VOCAB_MAP.keys())\n",
        "\n",
        "SOS_TOKEN = VOCAB_MAP[\"[SOS]\"]\n",
        "EOS_TOKEN = VOCAB_MAP[\"[EOS]\"]\n",
        "PAD_TOKEN = VOCAB_MAP[\"[PAD]\"]\n",
        "\n",
        "Y_train = [np.array([VOCAB_MAP[p] for p in seq]) for seq in Y_train]\n",
        "Y_valid = [np.array([VOCAB_MAP[p] for p in seq]) for seq in Y_valid]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNjjsFqpbocR"
      },
      "outputs": [],
      "source": [
        "# Dataset class for the Toy dataset\n",
        "class ToyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, partition):\n",
        "\n",
        "        if partition == \"train\":\n",
        "            self.mfccs = X_train[:, :, :15]\n",
        "            self.transcripts = Y_train\n",
        "\n",
        "        elif partition == \"valid\":\n",
        "            self.mfccs = X_valid[:, :, :15]\n",
        "            self.transcripts = Y_valid\n",
        "\n",
        "        assert len(self.mfccs) == len(self.transcripts)\n",
        "\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        x = torch.tensor(self.mfccs[i])\n",
        "        y = torch.tensor(self.transcripts[i])\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def collate_fn(batch):\n",
        "\n",
        "        x_batch, y_batch = list(zip(*batch))\n",
        "\n",
        "        x_lens      = [x.shape[0] for x in x_batch] \n",
        "        y_lens      = [y.shape[0] for y in y_batch] \n",
        "\n",
        "        x_batch_pad = torch.nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value= EOS_TOKEN)\n",
        "        y_batch_pad = torch.nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value= EOS_TOKEN) \n",
        "        \n",
        "        return x_batch_pad, y_batch_pad, torch.tensor(x_lens), torch.tensor(y_lens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pktcgc4FVe1b",
        "outputId": "26e81557-5822-4994-e5de-8794783e4ba6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  96\n",
            "Train dataset samples = 16000, batches = 167\n",
            "Val dataset samples = 1600, batches = 17\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = ToyDataset(partition=\"train\") #TODO\n",
        "val_data = ToyDataset(partition=\"valid\") # TODO : You can either use the same class with some modifications or make a new one :)\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = DataLoader(train_data, num_workers= 2,\n",
        "                                           batch_size=config['batch_size'], pin_memory=True,\n",
        "                                           shuffle=True, collate_fn=ToyDataset.collate_fn)\n",
        "val_loader = DataLoader(val_data, num_workers= 2,\n",
        "                                           batch_size=config['batch_size'], pin_memory=True,\n",
        "                                           shuffle=True, collate_fn=ToyDataset.collate_fn)\n",
        "\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sozuQg1GV0k-",
        "outputId": "b402f6f5-0131-4ee5-b10d-301133bc00bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([96, 176, 15]) torch.Size([96, 23]) torch.Size([96]) torch.Size([96])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWRjucnUdbQ1"
      },
      "source": [
        "# Kaggle Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fStTuAQ6XAuD",
        "outputId": "7054ff8b-d86b-45c3-e213-7e0869845ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 4.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73274 sha256=d82691f96fc8674c4cc7462da821884e8b5888f7d68ad5ff674d81dec8fdfcb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/67/7b/a6d668747974998471d29b230e7221dd01330ac34faebe4af4\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ],
      "source": [
        "# TODO: Use the same Kaggle code from HW1P2, HW2P2, HW3P2\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle/\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"ruidichang\",\"key\":\"ec93dcec34e885b48bb96b02e9b79b80\"}') # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR74ooCSa664",
        "outputId": "1d356b09-9a12-4454-9336-678a243594ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-f22-hw4p2.zip to /content\n",
            " 99% 2.06G/2.09G [00:14<00:00, 250MB/s]\n",
            "100% 2.09G/2.09G [00:14<00:00, 152MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the data\n",
        "!kaggle competitions download -c 11-785-f22-hw4p2\n",
        "!mkdir '/content/data'\n",
        "\n",
        "!unzip -qo '11-785-f22-hw4p2.zip' -d '/content/data'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5ioyn6ldQB9"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHix8UvpBWh7",
        "outputId": "8862b776-b2bf-4b20-83ea-c13d72d726b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'<sos>': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'H': 8, 'I': 9, 'J': 10, 'K': 11, 'L': 12, 'M': 13, 'N': 14, 'O': 15, 'P': 16, 'Q': 17, 'R': 18, 'S': 19, 'T': 20, 'U': 21, 'V': 22, 'W': 23, 'X': 24, 'Y': 25, 'Z': 26, \"'\": 27, ' ': 28, '<eos>': 29}\n",
            "30\n"
          ]
        }
      ],
      "source": [
        "# These are the various characters in the transcripts of the datasetW\n",
        "VOCAB = ['<sos>',   \n",
        "         'A',   'B',    'C',    'D',    \n",
        "         'E',   'F',    'G',    'H',    \n",
        "         'I',   'J',    'K',    'L',       \n",
        "         'M',   'N',    'O',    'P',    \n",
        "         'Q',   'R',    'S',    'T', \n",
        "         'U',   'V',    'W',    'X', \n",
        "         'Y',   'Z',    \"'\",    ' ', \n",
        "         '<eos>']\n",
        "\n",
        "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
        "\n",
        "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
        "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
        "print(VOCAB_MAP)\n",
        "print(len(VOCAB))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SndiVRVqBMa"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a dataset class which is exactly the same as HW3P2. You are free to reuse it. \n",
        "# The only change is that the transcript mapping is different for this HW.\n",
        "# Note: We also want to retain SOS and EOS tokens in the transcript this time.\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, VOCAB, path): \n",
        "\n",
        "        # Load the directory and all files in them\n",
        "        self.mfcc_dir = os.path.join(path,'mfcc')\n",
        "        self.transcript_dir = os.path.join(path,'transcript/raw')\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.vocab = VOCAB\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        assert len(self.mfcc_files) == len(self.transcript_files) # Making sure that we have the same no. of mfcc and transcripts\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "\n",
        "        for i in range(0, self.length):\n",
        "        #   Load a single mfcc\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir,self.mfcc_files[i]))\n",
        "        #   Optionally do Cepstral Normalization of mfcc\n",
        "        #   Load the corresponding transcript\n",
        "            transcript = np.load(os.path.join(self.transcript_dir,self.transcript_files[i]))\n",
        "            transcript = transcript[1:-1] # Remove [SOS] and [EOS] from the transcript (Is there an efficient way to do this \n",
        "            # without traversing through the transcript?)\n",
        "          \n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS  \n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append([self.vocab.index(t) for t in transcript])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return len(self.mfccs)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "        '''\n",
        "        mfcc = torch.tensor(self.mfccs[ind])\n",
        "        transcript = torch.tensor(self.transcripts[ind]) \n",
        "        return mfcc, transcript\n",
        "\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "\n",
        "        batch_mfcc, batch_transcript = list(zip(*batch))\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = torch.nn.utils.rnn.pad_sequence(batch_mfcc, batch_first=True, padding_value= EOS_TOKEN)\n",
        "        lengths_mfcc = [sample.shape[0] for sample in batch_mfcc]\n",
        "\n",
        "        batch_transcript_pad = torch.nn.utils.rnn.pad_sequence(\n",
        "            batch_transcript,  batch_first=True, padding_value=EOS_TOKEN)\n",
        "        lengths_transcript = [sample.shape[0] for sample in batch_transcript]\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zannKblcPORG"
      },
      "outputs": [],
      "source": [
        "# TODO: Similarly, create a test dataset class\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, VOCAB, path): \n",
        "\n",
        "        # Load the directory and all files in them\n",
        "        self.mfcc_dir = os.path.join(path,'mfcc')\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "\n",
        "        self.vocab = VOCAB\n",
        "        self.mfccs = []\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "\n",
        "        for i in range(0, self.length):\n",
        "        #   Load a single mfcc\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir,self.mfcc_files[i]))\n",
        "        #   Optionally do Cepstral Normalization of mfcc\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return len(self.mfccs)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "        '''\n",
        "        mfcc = torch.tensor(self.mfccs[ind])\n",
        "        return mfcc\n",
        "\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "\n",
        "        batch_mfcc = [mfcc for mfcc in batch]\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = torch.nn.utils.rnn.pad_sequence(batch_mfcc, batch_first=True, padding_value= EOS_TOKEN)\n",
        "        lengths_mfcc = [sample.shape[0] for sample in batch_mfcc]\n",
        "        \n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQenneVsDLnX"
      },
      "source": [
        "# Dataset and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_IyfIDjXDDC"
      },
      "outputs": [],
      "source": [
        "transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "\n",
        "path = '/content/data/hw4p2' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsXBTeM8XILz",
        "outputId": "ad407605-7d39-4f3e-c389-7bb4d1ef8279"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9K_mrZvXKQQ"
      },
      "outputs": [],
      "source": [
        "# TODO: Create the datasets and dataloaders\n",
        "# All these things are similar to HW3P2\n",
        "# You can reuse the same code\n",
        "\n",
        "# The sanity check for shapes also are similar\n",
        "# Please remember that the only change in the dataset for this HW is the transcripts\n",
        "# So you are expected to get similar shapes like HW3P2 (Pad, pack and Oh my!)\n",
        "train_data = AudioDataset(VOCAB, os.path.join(path,'train-clean-100')) \n",
        "val_data = AudioDataset(VOCAB, os.path.join(path,'dev-clean')) # TODO : You can either use the same class with some modifications or make a new one :)\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = DataLoader(train_data, num_workers= 2,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle= True,collate_fn=AudioDataset.collate_fn)\n",
        "val_loader = DataLoader(val_data, num_workers= 2,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle= False, collate_fn=AudioDataset.collate_fn)\n",
        "\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nPhrqL6XMMm"
      },
      "outputs": [],
      "source": [
        "test_data = AudioDatasetTest(VOCAB,os.path.join(path,'test-clean'))\n",
        "\n",
        "test_loader = DataLoader(test_data, num_workers= 2,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle= False, collate_fn=AudioDatasetTest.collate_fn)\n",
        "\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dCTpJuMrXN-K",
        "outputId": "c385cd56-45b4-4d79-b5fb-132de4bbf3b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([96, 1642, 15]) torch.Size([96, 291]) torch.Size([96]) torch.Size([96])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NP3WGcOBXQUP",
        "outputId": "4b640860-0e4e-4601-b70e-17b498b12e00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([96, 1047, 15]) torch.Size([96])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in test_loader:\n",
        "    t, lt = data\n",
        "    print(t.shape, lt.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I--VjKlEhwi8"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uql9E6cqROvJ"
      },
      "source": [
        "In this section you will be building the LAS model from scratch. Before starting to code, please read the writeup, paper and understand the following parts completely.<br>\n",
        "- Pyramidal Bi-LSTM \n",
        "- Listener\n",
        "- Attention\n",
        "- Speller\n",
        "\n",
        "After getting a good grasp of the workings of these modules, start coding. Follow the TODOs carefully. We will also be adding some extra features to the attention mechanism like keys and values which are not originally present in LAS. So we will be creating a hybrid network based on LAS and Attention is All You Need.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCbwz0LZMWwe"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoI0zEoIMX5I"
      },
      "source": [
        "### Pyramidal Bi-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-F9zAQR95P55"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "        (Tip: Write down the shapes and understand)\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "\n",
        "        self.blstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, bidirectional=True, batch_first=True)# TODO: Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
        "\n",
        "    def forward(self, x_packed, x_lens): # x_packed is a PackedSequence\n",
        "        # TODO: Pad Packed Sequence\n",
        "        # x, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n",
        "        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
        "        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
        "        x, x_lens = self.trunc_reshape(x_packed, x_lens)\n",
        "        # TODO: Pack Padded Sequence. What output(s) would you get?\n",
        "        x = pack_padded_sequence(x, x_lens/2, batch_first=True, enforce_sorted=False)\n",
        "        # TODO: Pass the sequence through bLSTM\n",
        "        output = self.blstm(x)[0]\n",
        "        output, output_len = pad_packed_sequence(output, batch_first=True)\n",
        "        # What do you return?\n",
        "        return output, output_len\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens): \n",
        "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "        if(x.shape[1]%2!=0):\n",
        "            x = x[:,:-1,:]\n",
        "        # TODO: Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "        x = x.reshape(x.shape[0], int(x.shape[1]/2), x.shape[2]*2)\n",
        "        # TODO: Reduce lengths by the same downsampling factor\n",
        "        return x, x_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rchbyjlMeB2"
      },
      "source": [
        "### Listener"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "912b3sVoHr1e"
      },
      "outputs": [],
      "source": [
        "class Listener(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, encoder_hidden_size):\n",
        "        super(Listener, self).__init__()\n",
        "        # The first LSTM at the very bottom\n",
        "        self.blstm = torch.nn.LSTM(input_size=input_size, hidden_size=encoder_hidden_size, num_layers=1, bidirectional=True, batch_first=True)#TODO: Fill this up\n",
        "\n",
        "        # self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?\n",
        "        #     # TODO: Fill this up with pBLSTMs - What should the input_size be? \n",
        "        #     # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
        "        #     # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n",
        "        #     pBLSTM(encoder_hidden_size * 4, encoder_hidden_size),\n",
        "        #     pBLSTM(encoder_hidden_size * 4, encoder_hidden_size),\n",
        "        #     pBLSTM(encoder_hidden_size * 4, encoder_hidden_size)\n",
        "        # )\n",
        "        self.pBLSTM1 = pBLSTM(encoder_hidden_size * 4, encoder_hidden_size)\n",
        "        self.pBLSTM2 = pBLSTM(encoder_hidden_size * 4, encoder_hidden_size)\n",
        "        self.pBLSTM3 = pBLSTM(encoder_hidden_size * 4, encoder_hidden_size)\n",
        "        self.lockeddropout = LockedDropout(p=0.2)\n",
        "         \n",
        "    def forward(self, x, x_lens):\n",
        "        # Where are x and x_lens coming from? The dataloader\n",
        "        # TODO: Pack Padded Sequence\n",
        "        packed_sequence = pack_padded_sequence(x, x_lens, enforce_sorted=False, batch_first=True)\n",
        "        # TODO: Pass it through the first LSTM layer (no truncation)\n",
        "        out, _ = self.blstm(packed_sequence)\n",
        "        # TODO: Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "        out, out_lens = pad_packed_sequence(out, batch_first=True)\n",
        "        encoder_outputs, encoder_lens = self.pBLSTM1(out, out_lens)\n",
        "        encoder_outputs = self.lockeddropout(encoder_outputs)\n",
        "        encoder_outputs, encoder_lens = self.pBLSTM2(encoder_outputs, encoder_lens)\n",
        "        encoder_outputs = self.lockeddropout(encoder_outputs)\n",
        "        encoder_outputs, encoder_lens = self.pBLSTM3(encoder_outputs, encoder_lens)\n",
        "        # TODO: Pad Packed Sequence\n",
        "        # encoder_outputs, encoder_lens = pad_packed_sequence(encoder_outputs, batch_first=True) \n",
        "        # Remember the number of output(s) each function returns\n",
        "        return encoder_outputs, encoder_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0EgRqsKDI7dD",
        "outputId": "e1d12e0d-5511-4a1f-9d92-4c3471f1fbdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Listener(\n",
            "  (blstm): LSTM(15, 256, batch_first=True, bidirectional=True)\n",
            "  (pBLSTM1): pBLSTM(\n",
            "    (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (pBLSTM2): pBLSTM(\n",
            "    (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (pBLSTM3): pBLSTM(\n",
            "    (blstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
            "  )\n",
            "  (lockeddropout): LockedDropout(p=0.2)\n",
            ")\n",
            "=======================================================================\n",
            "                     Kernel Shape    Output Shape     Params Mult-Adds\n",
            "Layer                                                                 \n",
            "0_blstm                         -   [124214, 512]   559.104k  555.008k\n",
            "1_pBLSTM1.LSTM_blstm            -    [62081, 512]  2.625536M  2.62144M\n",
            "2_lockeddropout                 -  [96, 821, 512]          -         -\n",
            "3_pBLSTM2.LSTM_blstm            -    [31015, 512]  2.625536M  2.62144M\n",
            "4_lockeddropout                 -  [96, 410, 512]          -         -\n",
            "5_pBLSTM3.LSTM_blstm            -    [15478, 512]  2.625536M  2.62144M\n",
            "-----------------------------------------------------------------------\n",
            "                         Totals\n",
            "Total params          8.435712M\n",
            "Trainable params      8.435712M\n",
            "Non-trainable params        0.0\n",
            "Mult-Adds             8.419328M\n",
            "=======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        }
      ],
      "source": [
        "encoder = Listener(15,256).to(DEVICE)# TODO: Initialize Listener\n",
        "print(encoder)\n",
        "summary(encoder, x.to(DEVICE), lx)\n",
        "del encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJCpBcEmMVcZ"
      },
      "source": [
        "## Attention (Attend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6k9R7jKMRcZ"
      },
      "source": [
        "Different ways to compute Attention\n",
        "\n",
        "1. Dot-product attention\n",
        "    * raw_weights = bmm(key, query) \n",
        "    * Optional: Scaled dot-product by normalizing with sqrt key dimension \n",
        "    * Check \"Attention is All You Need\" Section 3.2.1\n",
        "    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n",
        "\n",
        "\n",
        "2. Cosine attention\n",
        "    * raw_weights = cosine(query, key) # almost the same as dot-product xD \n",
        "\n",
        "3. Bi-linear attention\n",
        "    * W = Linear transformation (learnable parameter): d_k -> d_q\n",
        "    * raw_weights = bmm(key @ W, query)\n",
        "\n",
        "4. Multi-layer perceptron\n",
        "    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
        "\n",
        "5. Multi-Head Attention\n",
        "    * Check \"Attention is All You Need\" Section 3.2.2\n",
        "    * h = Number of heads\n",
        "    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "    * W_O: d_v -> d_v\n",
        "    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n",
        "    * raw_weights = Q @ K^T\n",
        "    * masked_raw_weights = mask(raw_weights)\n",
        "    * attention = softmax(masked_raw_weights)\n",
        "    * multi_head = attention @ V\n",
        "    * multi_head = multi_head reshaped to (B, d_v)\n",
        "    * context = multi_head @ W_O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqu-MUM8TjUO"
      },
      "outputs": [],
      "source": [
        "def plot_attention(attention): \n",
        "    # Function for plotting attention\n",
        "    # You need to get a diagonal plot\n",
        "    plt.clf()\n",
        "    sns.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using the key, value (from encoder hidden states) and query from decoder.\n",
        "    Here are different ways to compute attention and context:\n",
        "\n",
        "    After obtaining the raw weights, compute and return attention weights and context as follows.:\n",
        "\n",
        "    masked_raw_weights  = mask(raw_weights) # mask out padded elements with big negative number (e.g. -1e9 or -inf in FP16)\n",
        "    attention           = softmax(masked_raw_weights)\n",
        "    context             = bmm(attention, value)\n",
        "    \n",
        "    At the end, you can pass context through a linear layer too.\n",
        "\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, encoder_hidden_size, decoder_output_size, projection_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.key_projection     = torch.nn.Linear(encoder_hidden_size * 2, projection_size)# TODO: Define an nn.Linear layer which projects the encoder_hidden_state to keys\n",
        "        self.value_projection   = torch.nn.Linear(encoder_hidden_size * 2, projection_size)# TODO: Define an nn.Linear layer which projects the encoder_hidden_state to value\n",
        "        self.query_projection   = torch.nn.Linear(decoder_output_size , projection_size)# TODO: Define an nn.Linear layer which projects the decoder_output_state to query\n",
        "        # Optional : Define an nn.Linear layer which projects the context vector\n",
        "\n",
        "        self.softmax            = torch.nn.Softmax(dim=1)# TODO: Define a softmax layer. Think about the dimension which you need to apply \n",
        "        # Tip: What is the shape of energy? And what are those?\n",
        "\n",
        "    # As you know, in the attention mechanism, the key, value and mask are calculated only once.\n",
        "    # This function is used to calculate them and set them to self\n",
        "    def set_key_value_mask(self, encoder_outputs, encoder_lens):\n",
        "    \n",
        "        encoder_max_seq_len = encoder_outputs.shape[1]\n",
        "\n",
        "        self.key      = self.key_projection(encoder_outputs) # TODO: Project encoder_outputs using key_projection to get keys\n",
        "        self.value    = self.value_projection(encoder_outputs)# TODO: Project encoder_outputs using value_projection to get values\n",
        "\n",
        "        # encoder_max_seq_len is of shape (batch_size, ) which consists of the lengths encoder output sequences in that batch\n",
        "        # The raw_weights are of shape (batch_size, timesteps)\n",
        "\n",
        "        # TODO: To remove the influence of padding in the raw_weights, we want to create a boolean mask of shape (batch_size, timesteps) \n",
        "        # The mask is False for all indicies before padding begins, True for all indices after.\n",
        "\n",
        "        self.padding_mask     =  torch.arange(encoder_max_seq_len).unsqueeze(0) >= encoder_lens.unsqueeze(1)# TODO: You want to use a comparison between encoder_max_seq_len and encoder_lens to create this mask. \n",
        "        self.padding_mask     =  self.padding_mask.to(DEVICE)\n",
        "        # (Hint: Broadcasting gives you a one liner)\n",
        "        \n",
        "    def forward(self, decoder_output_embedding):\n",
        "        # key   : (batch_size, timesteps, projection_size)\n",
        "        # value : (batch_size, timesteps, projection_size)\n",
        "        # query : (batch_size, projection_size)\n",
        "\n",
        "        self.query         = self.query_projection(decoder_output_embedding)# TODO: Project the query using query_projection\n",
        "        # Hint: Take a look at torch.bmm for the products below \n",
        "\n",
        "        raw_weights        = (torch.bmm(self.key, self.query.unsqueeze(2))).squeeze(2)# TODO: Calculate raw_weights which is the product of query and key, and is of shape (batch_size, timesteps)\n",
        "        _MASKING_VALUE     = -1e+30 if raw_weights.dtype == torch.float32 else -1e+4\n",
        "        masked_raw_weights = raw_weights.masked_fill_(self.padding_mask, _MASKING_VALUE) # TODO: Mask the raw_weights with self.padding_mask. \n",
        "        # Take a look at pytorch's masked_fill_ function (You want the fill value to be a big negative number for the softmax to make it close to 0)\n",
        "\n",
        "        attention_weights  = self.softmax(masked_raw_weights)# TODO: Calculate the attention weights, which is the softmax of raw_weights\n",
        "        context            = torch.bmm(attention_weights.unsqueeze(1), self.value).squeeze(1)# TODO: Calculate the context - it is a product between attention_weights and value\n",
        "\n",
        "        # Hint: You might need to use squeeze/unsqueeze to make sure that your operations work with bmm\n",
        "        # print(\"context size:\",context.shape)\n",
        "        return context, attention_weights # Return the context, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78XOdWExMSi-"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuUQTy2NMlbT"
      },
      "source": [
        "### Speller"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7-R6BTuT8dm"
      },
      "outputs": [],
      "source": [
        "class Speller(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module= None):\n",
        "        super(Speller, self).__init__()\n",
        "\n",
        "        self.vocab_size         = vocab_size\n",
        "\n",
        "        self.decoder_output_size = decoder_output_size\n",
        "\n",
        "        self.embedding          = torch.nn.Embedding(vocab_size, embed_size, padding_idx=0)# TODO: Initialize the Embedding Layer (Use the nn.Embedding Layer from torch), make sure you set the correct padding_idx  \n",
        "\n",
        "        self.lstm_cell1         = torch.nn.LSTMCell(input_size=embed_size+decoder_output_size, hidden_size=decoder_hidden_size)\n",
        "                                      \n",
        "        self.lstm_cell2         = torch.nn.LSTMCell(input_size=decoder_hidden_size, hidden_size=decoder_output_size)\n",
        "        \n",
        "        self.lockeddropout      = LockedDropout(p=0.2)\n",
        "\n",
        "        self.char_prob          = torch.nn.Linear(decoder_output_size+decoder_output_size, vocab_size)# TODO: Initialize the classification layer to generate your probability distribution over all characters\n",
        "\n",
        "        self.char_prob.weight   = self.embedding.weight # Weight tying\n",
        "\n",
        "        self.attention          = attention_module\n",
        "\n",
        "    \n",
        "    def forward(self, encoder_outputs, encoder_lens, y = None, tf_rate = 1): \n",
        "\n",
        "        '''\n",
        "        Args: \n",
        "            embedding: Attention embeddings \n",
        "            hidden_list: List of Hidden States for the LSTM Cells\n",
        "        ''' \n",
        "      \n",
        "        batch_size, encoder_max_seq_len, _ = encoder_outputs.shape\n",
        "        # print(encoder_outputs.shape)\n",
        "        # print(encoder_lens.shape)\n",
        "        if self.training:\n",
        "            timesteps     = y.shape[1] # The number of timesteps is the sequence of length of your transcript during training\n",
        "            label_embed   = self.embedding(y) # Embeddings of the transcript, when we want to use teacher forcing\n",
        "        else:\n",
        "            timesteps     = 600 # 600 is a design choice that we recommend, however you are free to experiment.\n",
        "\n",
        "        # INITS\n",
        "        predictions     = []\n",
        "        prediction      = torch.zeros(batch_size, 1).to(DEVICE)\n",
        "\n",
        "        # Initialize the first character input to your decoder, SOS\n",
        "        char            = torch.full((batch_size,), fill_value=SOS_TOKEN, dtype= torch.long).to(DEVICE) \n",
        "\n",
        "        # Initialize a list to keep track of LSTM Cell Hidden and Cell Memory States, to None\n",
        "        hidden_states   = [None,None]\n",
        "\n",
        "        attention_plot          = []\n",
        "        context                 = torch.zeros(batch_size, self.decoder_output_size).to(DEVICE)# TODO: Initialize context (You have a few choices, refer to the writeup )\n",
        "        attention_weights       = torch.zeros(batch_size, encoder_max_seq_len) # Attention Weights are zero if not using Attend Module\n",
        "        # Set Attention Key, Value, Padding Mask just once\n",
        "        if self.attention != None:\n",
        "            self.attention.set_key_value_mask(encoder_outputs, encoder_lens)\n",
        "\n",
        "        for t in range(timesteps):\n",
        "            \n",
        "            if self.training and t > 0:\n",
        "                # TODO: We want to decide which embedding to use as input for the decoder during training\n",
        "                # We can use the embedding of the transcript character or the embedding of decoded/predicted character, from the previous timestep \n",
        "                # Using the embedding of the transcript character is teacher forcing, it is very important for faster convergence\n",
        "                # Use a comparison between a random probability and your teacher forcing rate, to decide which embedding to use\n",
        "                if np.random.random() > tf_rate:\n",
        "                    prediction = torch.nn.functional.gumbel_softmax(prediction) # Gumbel noise\n",
        "                    char_embed = self.embedding(prediction.argmax(dim=-1)).to(DEVICE)\n",
        "                # Teacher Forcing\n",
        "                else:\n",
        "                    char_embed = label_embed[:,t-1].to(DEVICE)\n",
        "            else:\n",
        "                char_embed = self.embedding(char).to(DEVICE)#TODO: Generate the embedding for the character at timestep t\n",
        "            # print(\"char_embed shape\", char_embed.shape)\n",
        "            decoder_input_embedding = torch.cat([char_embed, context], dim=1)# TODO: What do we want to concatenate as input to the decoder? (Use torch.cat)\n",
        "            \n",
        "            # print(\"decoder input shape:\", decoder_input_embedding.shape )\n",
        "            # Loop over your lstm cells\n",
        "            # Each lstm cell takes in an embedding \n",
        "            # print(decoder_input_embedding.shape)\n",
        "\n",
        "            hidden_states[0] = self.lstm_cell1(decoder_input_embedding, hidden_states[0]) \n",
        "            decoder_input_embedding = hidden_states[0][0]\n",
        "            decoder_input_embedding = self.lockeddropout(decoder_input_embedding.unsqueeze(0)).squeeze(0)\n",
        "            hidden_states[1] = self.lstm_cell2(decoder_input_embedding, hidden_states[1]) \n",
        "            decoder_input_embedding = hidden_states[1][0]\n",
        "            # The output embedding from the decoder is the hidden state of the last LSTM Cell\n",
        "            decoder_output_embedding = hidden_states[-1][0]\n",
        "            \n",
        "            # We compute attention from the output of the last LSTM Cell\n",
        "            if self.attention != None:\n",
        "                context, attention_weights = self.attention(decoder_output_embedding) # The returned query is the projected query\n",
        "\n",
        "            attention_plot.append(attention_weights[0].detach().cpu())\n",
        "            output_embedding     = torch.cat([hidden_states[-1][0], context], dim=1)# TODO: Concatenate the projected query with context for the output embedding\n",
        "            # Hint: How can you get the projected query from attention\n",
        "            # If you are not using attention, what will you use instead of query?\n",
        "            # print(\"shape after cat:\",output_embedding.shape)\n",
        "            # print(output_embedding.shape)\n",
        "            # print(self.char_prob.weight.shape)\n",
        "            prediction            = self.char_prob(output_embedding)\n",
        "            \n",
        "            # Append the character probability distribution to the list of predictions \n",
        "            predictions.append(prediction)\n",
        "            # print(\"prediction shape\", len(predictions))\n",
        "            char = torch.argmax(prediction, dim=-1)# TODO: Get the predicted character for the next timestep from the probability distribution \n",
        "            # (Hint: Use Greedy Decoding for starters)\n",
        "\n",
        "        attention_plot  = torch.stack(attention_plot, dim=1)# TODO: Stack list of attetion_plots \n",
        "        predictions     = torch.stack(predictions, dim=1)# TODO: Stack list of predictions \n",
        "        # print(\"final shape\",predictions.shape)\n",
        "        return predictions, attention_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMgncQmVMnCO"
      },
      "source": [
        "## Sequence-to-Sequence Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWWzurvXM0iv"
      },
      "source": [
        "### LAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcTC4cK95TYT"
      },
      "outputs": [],
      "source": [
        "class LAS(torch.nn.Module):\n",
        "    def __init__(self, input_size, encoder_hidden_size,\n",
        "                 vocab_size, embed_size,\n",
        "                 decoder_hidden_size, decoder_output_size,\n",
        "                 projection_size= 128):\n",
        "        \n",
        "        super(LAS, self).__init__()\n",
        "\n",
        "        self.encoder        = Listener(input_size, encoder_hidden_size)# TODO: Initialize Encoder\n",
        "        attention_module    = Attention(encoder_hidden_size, decoder_output_size, projection_size)# TODO: Initialize Attention\n",
        "        self.decoder        = Speller(embed_size, decoder_hidden_size, decoder_output_size, vocab_size, attention_module = attention_module)# TODO: Initialize Decoder, make sure you pass the attention module \n",
        "\n",
        "    def forward(self, x, x_lens, y = None, tf_rate = 1):\n",
        "\n",
        "        encoder_outputs, encoder_lens = self.encoder(x, x_lens) # from Listener\n",
        "        predictions, attention_plot = self.decoder(encoder_outputs, encoder_lens, y, tf_rate)\n",
        "        return predictions, attention_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHMzR6fLht5n"
      },
      "source": [
        "# Training Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TI2AKhQ6YP6F"
      },
      "source": [
        "## Model Setup\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS-YUHQlYQmL"
      },
      "outputs": [],
      "source": [
        "# Baseline LAS has the following configuration:\n",
        "# Encoder bLSTM/pbLSTM Hidden Dimension of 512 (256 per direction)\n",
        "# Decoder Embedding Layer Dimension of 256\n",
        "# Decoder Hidden Dimension of 512 \n",
        "# Decoder Output Dimension of 128\n",
        "# Attention Projection Size of 128\n",
        "# Feel Free to Experiment with this \n",
        "model = LAS(\n",
        "    input_size=15, encoder_hidden_size=512,\n",
        "                 vocab_size=len(VOCAB), embed_size=256,\n",
        "                 decoder_hidden_size=512, decoder_output_size=128,\n",
        "                 projection_size= 128\n",
        "    # Initialize your model \n",
        "    # Read the paper and think about what dimensions should be used\n",
        "    # You can experiment on these as well, but they are not requried for the early submission\n",
        "    # Remember that if you are using weight tying, some sizes need to be the same\n",
        ").to(DEVICE)\n",
        "\n",
        "summary(model, x= x.to(DEVICE), x_lens= lx, y= y.to(DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDmcYul-YSdC"
      },
      "source": [
        "## Optimizer, Scheduler, Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HwmgDSvbtmd"
      },
      "outputs": [],
      "source": [
        "optimizer   = torch.optim.Adam(model.parameters(), lr= 0.001, amsgrad= True, weight_decay= 5e-6)\n",
        "criterion   = torch.nn.CrossEntropyLoss(reduction='none') # Why are we using reduction = 'none' ? \n",
        "scaler      = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Optional: Create a custom class for a Teacher Force Schedule \n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baHCja89YV-m"
      },
      "source": [
        "# Levenshtein Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDYZnnLbqJ8J"
      },
      "outputs": [],
      "source": [
        "# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n",
        "def indices_to_chars(indices, vocab):\n",
        "    tokens = []\n",
        "    for i in indices: # This loops through all the indices\n",
        "        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n",
        "            continue\n",
        "        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n",
        "            break\n",
        "        else:\n",
        "            tokens.append(vocab[i])\n",
        "    return tokens\n",
        "\n",
        "# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n",
        "def calc_edit_distance(predictions, y, ly, vocab= VOCAB, print_example= False):\n",
        "\n",
        "    dist                = 0\n",
        "    batch_size, seq_len = predictions.shape\n",
        "\n",
        "    for batch_idx in range(batch_size): \n",
        "\n",
        "        y_sliced    = indices_to_chars(y[batch_idx,0:ly[batch_idx]], vocab)\n",
        "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
        "\n",
        "        # Strings - When you are using characters from the AudioDataset\n",
        "        y_string    = ''.join(y_sliced)\n",
        "        pred_string = ''.join(pred_sliced)\n",
        "        \n",
        "        dist        += Levenshtein.distance(pred_string, y_string)\n",
        "        # Comment the above and uncomment below for toy dataset \n",
        "        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
        "\n",
        "    if print_example: \n",
        "        # Print y_sliced and pred_sliced if you are using the toy dataset\n",
        "        print(\"Ground Truth : \", y_string)\n",
        "        print(\"Prediction   : \", pred_string)\n",
        "        \n",
        "    dist/=batch_size\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zjfF88iZ4Nc"
      },
      "source": [
        "# Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIXzhQclhs98"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=True, position=0, desc='Train')\n",
        "\n",
        "    running_loss        = 0.0\n",
        "    running_perplexity  = 0.0\n",
        "    \n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            predictions, attention_plot = model(x, lx, y= y, tf_rate= teacher_forcing_rate)\n",
        "\n",
        "            # Predictions are of Shape (batch_size, timesteps, vocab_size). \n",
        "            batch_size, timesteps, vocab_size = predictions.shape\n",
        "\n",
        "            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n",
        "            # So in total, you have batch_size*timesteps amount of characters.\n",
        "            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n",
        "            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n",
        "            loss        = criterion(predictions.reshape(-1, vocab_size), y.reshape(-1)).to(DEVICE)# TODO: Cross Entropy Loss\n",
        "            mask        = torch.arange(y.size(1)).unsqueeze(0) <= (ly).unsqueeze(1)# TODO: Create a boolean mask using the lengths of your transcript that remove the influence of padding indices (in transcripts) in the loss \n",
        "            mask        = mask.to(DEVICE)\n",
        "            masked_loss = torch.sum(loss * mask.view(-1)) / torch.sum(mask)# Product between the mask and the loss, divided by the mask's sum. Hint: You may want to reshape the mask too \n",
        "            perplexity  = torch.exp(masked_loss) # Perplexity is defined the exponential of the loss\n",
        "\n",
        "            running_loss        += masked_loss.item()\n",
        "            running_perplexity  += perplexity.item()\n",
        "        \n",
        "        # Backward on the masked loss\n",
        "        scaler.scale(masked_loss).backward()\n",
        "\n",
        "        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n",
        "        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(running_loss/(i+1)),\n",
        "            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
        "            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    running_loss /= len(dataloader)\n",
        "    running_perplexity /= len(dataloader)\n",
        "    batch_bar.close()\n",
        "\n",
        "    return running_loss, running_perplexity, attention_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jzpCjd9R5VYV"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=True, desc=\"Val\")\n",
        "\n",
        "    running_lev_dist = 0.0\n",
        "\n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            predictions, attentions = model(x, lx, y = None)\n",
        "\n",
        "        # Greedy Decoding\n",
        "        greedy_predictions   =  torch.argmax(predictions,dim=2)# TODO: How do you get the most likely character from each distribution in the batch?\n",
        "\n",
        "        # Calculate Levenshtein Distance\n",
        "        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    running_lev_dist /= len(dataloader)\n",
        "\n",
        "    return running_lev_dist#, running_loss, running_perplexity, "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99rJQUdPkCUq"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s12S_bPMcguA"
      },
      "outputs": [],
      "source": [
        "best_lev_dist = float(\"inf\")\n",
        "tf_rate = 1\n",
        "\n",
        "for epoch in range(0, 30):\n",
        "    \n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    # Call train and validate \n",
        "    running_loss, running_perplexity, attention_plot = train(model, train_loader, criterion, optimizer, tf_rate)\n",
        "    running_lev_dist = validate(model, val_loader)\n",
        "\n",
        "    # Print your metrics\n",
        "\n",
        "    # Plot Attention \n",
        "    plot_attention(attention_plot)\n",
        "\n",
        "    # Log metrics to Wandb\n",
        "\n",
        "    # Optional: Scheduler Step / Teacher Force Schedule Step\n",
        "    scheduler.step(running_loss)\n",
        "\n",
        "    if epoch>10:\n",
        "      tf_rate -= 0.01\n",
        "\n",
        "    if running_lev_dist <= best_lev_dist:\n",
        "        best_lev_dist = running_lev_dist\n",
        "        print('saving...')\n",
        "        # Save your model checkpoint here\n",
        "        torch.save(model.state_dict(), \"model_best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3iickk_kJNB"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7HihzA4ViiR",
        "outputId": "d8f19129-aea6-4635-c677-063aa3ff4e3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Optional: Load your best model Checkpoint here\n",
        "model.load_state_dict(torch.load('/content/model_best.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTBD49c_kKs3"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a testing function similar to validation \n",
        "def indices_to_string(indices, VOCAB):\n",
        "    tokens = \"\"\n",
        "    for i in indices:\n",
        "        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n",
        "            continue\n",
        "        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n",
        "            break\n",
        "        else:\n",
        "            tokens += VOCAB[i]\n",
        "    return tokens\n",
        "\n",
        "def test(model, dataloader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        for i, (x, lx) in enumerate(dataloader):\n",
        "\n",
        "            x, lx = x.to(DEVICE), lx\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                predictions, attentions = model(x, lx, y = None)\n",
        "\n",
        "            # Greedy Decoding\n",
        "            greedy_predictions   =  torch.argmax(predictions,dim=2)# TODO: How do you get the most likely character from each distribution in the batch?\n",
        "            print(greedy_predictions.shape)\n",
        "            for i in range(len(x)):\n",
        "                preds.append(indices_to_string(greedy_predictions[i], VOCAB))\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "dcjzQHZat0Nb",
        "outputId": "a11b4e6a-da06-4463-89f4-39aaa5676e1d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6d847627efb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
          ]
        }
      ],
      "source": [
        "preds = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "JtD9jG_6esQF",
        "outputId": "f9d83a36-1d34-4801-b4cc-3f1788f4acac"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-63650d52bcba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id,label\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m','\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
          ]
        }
      ],
      "source": [
        "# TODO: Create a file with all predictions \n",
        "with open('submission.csv', 'w') as f:\n",
        "  f.write('id,label\\n')\n",
        "  for i in range(len(preds)):\n",
        "    f.write(str(i) + ',' + preds[i]+\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNcfy6LbsYP0",
        "outputId": "257a5f89-1141-4537-a18b-3793501ca6c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2620\n"
          ]
        }
      ],
      "source": [
        "print(len(preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lMp4bekfH-l",
        "outputId": "bf3baa71-76ca-49f6-fe5d-fb3e609d2ed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 64.2k/64.2k [00:00<00:00, 99.4kB/s]\n",
            "Successfully submitted to Attention-Based Speech Recognition"
          ]
        }
      ],
      "source": [
        "# TODO: Submit to Kaggle\n",
        "!kaggle competitions submit -c 11-785-f22-hw4p2 -f submission.csv -m \"I made it!\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "I8a_cZqr-UpV",
        "lr4xGzRU-KZz",
        "OALQCI0EDCwh",
        "X7J4sY1OW9Pr",
        "YWRjucnUdbQ1",
        "i5ioyn6ldQB9",
        "gQenneVsDLnX",
        "XoI0zEoIMX5I",
        "JJCpBcEmMVcZ",
        "LWWzurvXM0iv",
        "baHCja89YV-m",
        "1zjfF88iZ4Nc"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}