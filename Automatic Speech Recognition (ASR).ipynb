{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2_7O30yV_IQ"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "### Levenshtein\n",
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "3dd591a0-a8dc-4d7f-a7e0-a19f9f681e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (0.20.8)\n",
            "Requirement already satisfied: Levenshtein==0.20.8 in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein) (0.20.8)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from Levenshtein==0.20.8->python-Levenshtein) (2.13.2)\n",
            "fatal: destination path 'ctcdecode' already exists and is not an empty directory.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n",
            "/content/ctcdecode\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/ctcdecode\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: ctcdecode\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ctcdecode: filename=ctcdecode-1.0.3-cp37-cp37m-linux_x86_64.whl size=13405118 sha256=4d67ba54cdc5d3adf79a7649095b9de50694dcb37e972b44f464301b35dd6e87\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-19v5_oc1/wheels/da/bb/b4/233de9fd7927245208e27bcf688bf5680ae3f3874be2895eef\n",
            "Successfully built ctcdecode\n",
            "Installing collected packages: ctcdecode\n",
            "  Attempting uninstall: ctcdecode\n",
            "    Found existing installation: ctcdecode 1.0.3\n",
            "    Uninstalling ctcdecode-1.0.3:\n",
            "      Successfully uninstalled ctcdecode-1.0.3\n",
            "Successfully installed ctcdecode-1.0.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ctcdecode"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummaryX in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.3.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-Levenshtein\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget\n",
        "%cd ctcdecode\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "dfb62fc8-a844-45e3-b5c5-45fec8d6b722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "import wandb\n",
        "import time\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utqv6Aa8Xvlp",
        "outputId": "c260d670-c1dc-4a6c-844f-0514893542e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUelfGhom1m",
        "outputId": "b852b9ff-29b0-4ddc-f777-a6a94cff130f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Using cached kaggle-1.5.8-py3-none-any.whl\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.8\n",
            "    Uninstalling kaggle-1.5.8:\n",
            "      Successfully uninstalled kaggle-1.5.8\n",
            "Successfully installed kaggle-1.5.8\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"ruidichang\",\"key\":\"ec93dcec34e885b48bb96b02e9b79b80\"}') # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCpMyu3sXS_e",
        "outputId": "7f8b1371-df8b-44ac-d657-455b75fee83f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11-785-f22-hw3p2.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "replace hw3p2/dev-clean/mfcc/0084_121123_000.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: 11-785-f22-hw3p2.zip  ctcdecode  drive\thw3p2  sample_data\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n",
        "'''\n",
        "\n",
        "!kaggle competitions download -c 11-785-f22-hw3p2\n",
        "!unzip -q 11-785-f22-hw3p2.zip\n",
        "\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      },
      "source": [
        "## Model Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "# This overwrites the phonetics.py file.\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \", # BLANK TOKEN\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict\n",
        "mapping = CMUdict_ARPAbet\n",
        "LABELS = ARPAbet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ-qQ_Sf-LIu",
        "outputId": "24b01692-05d5-49d2-b0cd-b0d432bfbb74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "OUT_SIZE = len(LABELS)\n",
        "OUT_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"beam_width\" : 30,\n",
        "    \"lr\" : 0.00005,\n",
        "    \"epochs\" : 50,\n",
        "    \"batch_size\" : 64\n",
        "    } # Feel free to add more items here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN2kcxwXLLBb"
      },
      "outputs": [],
      "source": [
        "# You might want to play around with the mapping as a sanity check here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "## Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, PHONEMES): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "        path = '/content/hw3p2/train-clean-360/'\n",
        "        self.mfcc_dir = os.path.join(path,'mfcc')\n",
        "        self.transcript_dir = os.path.join(path,'transcript/raw')\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.phonemes = PHONEMES\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        assert len(self.mfcc_files) == len(self.transcript_files) # Making sure that we have the same no. of mfcc and transcripts\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "\n",
        "        for i in range(0, self.length):\n",
        "        #   Load a single mfcc\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir,self.mfcc_files[i]))\n",
        "        #   Optionally do Cepstral Normalization of mfcc\n",
        "        #   Load the corresponding transcript\n",
        "            transcript = np.load(os.path.join(self.transcript_dir,self.transcript_files[i]))\n",
        "            transcript = transcript[1:-1] # Remove [SOS] and [EOS] from the transcript (Is there an efficient way to do this \n",
        "            # without traversing through the transcript?)\n",
        "        \n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS  \n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append([self.phonemes.index(t) for t in transcript])\n",
        "\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "        path = '/content/hw3p2/train-clean-100/'\n",
        "        self.mfcc_dir = os.path.join(path,'mfcc')\n",
        "        self.transcript_dir = os.path.join(path,'transcript/raw')\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        assert len(self.mfcc_files) == len(self.transcript_files) # Making sure that we have the same no. of mfcc and transcripts\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "\n",
        "        for i in range(0, self.length):\n",
        "        #   Load a single mfcc\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir,self.mfcc_files[i]))\n",
        "        #   Optionally do Cepstral Normalization of mfcc\n",
        "        #   Load the corresponding transcript\n",
        "            transcript = np.load(os.path.join(self.transcript_dir,self.transcript_files[i]))\n",
        "            transcript = transcript[1:-1] # Remove [SOS] and [EOS] from the transcript (Is there an efficient way to do this \n",
        "            # without traversing through the transcript?)\n",
        "        \n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS  \n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append([self.phonemes.index(t) for t in transcript])\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return len(self.mfccs)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "\n",
        "        mfcc = torch.tensor(self.mfccs[ind])\n",
        "        transcript = torch.tensor(self.transcripts[ind]) \n",
        "        return mfcc, transcript\n",
        "\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [mfcc for mfcc,transcript in batch]\n",
        "        # batch of output phonemes\n",
        "        batch_transcript = [transcript for mfcc,transcript in batch]\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc)\n",
        "        lengths_mfcc = [sample.shape[0] for sample in batch_mfcc]\n",
        "\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True)\n",
        "        lengths_transcript = [sample.shape[0] for sample in batch_transcript]\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2OqFJkLh70O"
      },
      "source": [
        "## Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0phzUyl4h6a-"
      },
      "outputs": [],
      "source": [
        "class AudioDatasetValidation(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self,PHONEMES): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "        path = '/content/hw3p2/dev-clean/'\n",
        "        self.mfcc_dir = os.path.join(path,'mfcc')\n",
        "        self.transcript_dir = os.path.join(path,'transcript/raw')\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.phonemes = PHONEMES\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        assert len(self.mfcc_files) == len(self.transcript_files) # Making sure that we have the same no. of mfcc and transcripts\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "        \n",
        "        for i in range(0, self.length):\n",
        "        #   Load a single mfcc\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir,self.mfcc_files[i]))\n",
        "        #   Optionally do Cepstral Normalization of mfcc\n",
        "        #   Load the corresponding transcript\n",
        "            transcript = np.load(os.path.join(self.transcript_dir,self.transcript_files[i]))\n",
        "            transcript = transcript[1:len(transcript)-1] # Remove [SOS] and [EOS] from the transcript (Is there an efficient way to do this \n",
        "            # without traversing through the transcript?)\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append([self.phonemes.index(t) for t in transcript])\n",
        "\n",
        "        #TODO\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "        # self.transcripts = torch.LongTensor(np.asarray([self.phonemes.index(t) for t in self.transcripts[1:-1]]))\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return len(self.mfccs)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "\n",
        "        mfcc = torch.tensor(self.mfccs[ind])\n",
        "        transcript = torch.tensor(self.transcripts[ind]) \n",
        "        return mfcc, transcript\n",
        "\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [mfcc for mfcc,transcript in batch]\n",
        "        # batch of output phonemes\n",
        "        batch_transcript = [transcript for mfcc,transcript in batch]\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc)\n",
        "        lengths_mfcc = [sample.shape[0] for sample in batch_mfcc]\n",
        "\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True)\n",
        "        lengths_transcript = [sample.shape[0] for sample in batch_transcript]\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "## Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "#TODO\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "        path = '/content/hw3p2/test-clean/'\n",
        "        self.mfcc_dir = os.path.join(path,'mfcc')\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        # self.mfcc_files = os.listdir(self.mfcc_dir)\n",
        "\n",
        "        self.mfccs = []\n",
        "\n",
        "        #TODO\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        #TODO\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "        \n",
        "        for i in range(0, self.length):\n",
        "        #   Load a single mfcc\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir,self.mfcc_files[i]))\n",
        "        #   Optionally do Cepstral Normalization of mfcc\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return len(self.mfccs)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "\n",
        "        mfcc = torch.tensor(self.mfccs[ind])\n",
        "        return mfcc\n",
        "\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [mfcc for mfcc in batch]\n",
        "        \n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc)\n",
        "        lengths_mfcc = [sample.shape[0] for sample in batch_mfcc]\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "## Data - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "outputs": [],
      "source": [
        "transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "\n",
        "root = '/content/hw3p2' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "## Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "0d2b7c69-fa8c-450d-8a8a-69b4b95f61a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "195"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "27b9b757-be1f-48e0-9d05-39f2ddee2c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  64\n",
            "Train dataset samples = 132553, batches = 2072\n",
            "Val dataset samples = 2703, batches = 43\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset(PHONEMES) #TODO\n",
        "val_data = AudioDatasetValidation(PHONEMES) # TODO : You can either use the same class with some modifications or make a new one :)\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = DataLoader(train_data, num_workers= 2,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle= True,collate_fn=AudioDataset.collate_fn)\n",
        "val_loader = DataLoader(val_data, num_workers= 2,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle= True, collate_fn=AudioDatasetValidation.collate_fn)\n",
        "\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60xmQSAt0U2B",
        "outputId": "b602f1c4-15e9-4d79-b27a-47df22419456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test dataset samples = 2620, batches = 41\n"
          ]
        }
      ],
      "source": [
        "test_data = AudioDatasetTest() #TODO\n",
        "test_loader = DataLoader(test_data, num_workers= 2,\n",
        "                                           batch_size=config['batch_size'], pin_memory= True,\n",
        "                                           shuffle= False,collate_fn=AudioDatasetTest.collate_fn)\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "452082c7-750f-45d3-927f-c8b09b9a2e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1630, 64, 15]) torch.Size([64, 214]) torch.Size([64]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObIA-dMq_rDN",
        "outputId": "62562c8d-12e2-417b-b5dd-f173acd7322a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([825, 64, 15]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in test_loader:\n",
        "    t, lt = data\n",
        "    print(t.shape, lt.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "## wandb\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDduMaDIARE",
        "outputId": "295d2771-7ecf-4643-8dec-be03d089be7a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"f12c6cea0bcd560db008417ca06230259ed77e61\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "4s52yBOvICPZ",
        "outputId": "ffb56919-bad8-4ba0-9ffc-f12952935ba6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mruidichang\u001b[0m (\u001b[33mdlp_team_2022\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221119_023047-25szzd3q</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/dlp_team_2022/hw3p2-ablations/runs/25szzd3q\" target=\"_blank\">early-submission</a></strong> to <a href=\"https://wandb.ai/dlp_team_2022/hw3p2-ablations\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"early-submission\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account \n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyfQ4j7AbCcg"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t58_8VB1MaLA"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.cnn = torch.nn.Sequential(\n",
        "            nn.BatchNorm1d(15), \n",
        "            nn.Conv1d(15, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "          )\n",
        "        self.lstm = nn.LSTM(input_size=256, hidden_size=256, num_layers=3, bidirectional=True, dropout=0.3)\n",
        "        self.fc1 = nn.Linear(512, 43)\n",
        "    \n",
        "    def forward(self, X, lengths):\n",
        "        X = self.cnn(X.permute(1, 2, 0).contiguous()).permute(2, 0, 1)\n",
        "        packed_X = pack_padded_sequence(X, lengths, enforce_sorted=False)\n",
        "        packed_out = self.lstm(packed_X)[0]\n",
        "        out, out_lens = pad_packed_sequence(packed_out)\n",
        "        out = self.fc1(out).log_softmax(2)\n",
        "        return out, out_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "## INIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "CGoiXd70tb5z",
        "outputId": "a7749cbf-8975-41b3-9f82-8212cc3e20ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================================================================\n",
            "                     Kernel Shape     Output Shape     Params  Mult-Adds\n",
            "Layer                                                                   \n",
            "0_cnn.BatchNorm1d_0          [15]   [64, 15, 1630]       30.0       15.0\n",
            "1_cnn.Conv1d_1       [15, 256, 3]  [64, 256, 1630]    11.776k   18.7776M\n",
            "2_cnn.BatchNorm1d_2         [256]  [64, 256, 1630]      512.0      256.0\n",
            "3_cnn.ReLU_3                    -  [64, 256, 1630]          -          -\n",
            "4_lstm                          -     [77396, 512]  4.206592M  4.194304M\n",
            "5_fc1                   [512, 43]   [1630, 64, 43]    22.059k    22.016k\n",
            "-------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params           4.240969M\n",
            "Trainable params       4.240969M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds             22.994191M\n",
            "=========================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-e84ff311-a927-4bc4-b840-c5d1b6b368a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_cnn.BatchNorm1d_0</th>\n",
              "      <td>[15]</td>\n",
              "      <td>[64, 15, 1630]</td>\n",
              "      <td>30.0</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_cnn.Conv1d_1</th>\n",
              "      <td>[15, 256, 3]</td>\n",
              "      <td>[64, 256, 1630]</td>\n",
              "      <td>11776.0</td>\n",
              "      <td>18777600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_cnn.BatchNorm1d_2</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[64, 256, 1630]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_cnn.ReLU_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 256, 1630]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_lstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[77396, 512]</td>\n",
              "      <td>4206592.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_fc1</th>\n",
              "      <td>[512, 43]</td>\n",
              "      <td>[1630, 64, 43]</td>\n",
              "      <td>22059.0</td>\n",
              "      <td>22016.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e84ff311-a927-4bc4-b840-c5d1b6b368a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e84ff311-a927-4bc4-b840-c5d1b6b368a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e84ff311-a927-4bc4-b840-c5d1b6b368a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                     Kernel Shape     Output Shape     Params   Mult-Adds\n",
              "Layer                                                                    \n",
              "0_cnn.BatchNorm1d_0          [15]   [64, 15, 1630]       30.0        15.0\n",
              "1_cnn.Conv1d_1       [15, 256, 3]  [64, 256, 1630]    11776.0  18777600.0\n",
              "2_cnn.BatchNorm1d_2         [256]  [64, 256, 1630]      512.0       256.0\n",
              "3_cnn.ReLU_3                    -  [64, 256, 1630]        NaN         NaN\n",
              "4_lstm                          -     [77396, 512]  4206592.0   4194304.0\n",
              "5_fc1                   [512, 43]   [1630, 64, 43]    22059.0     22016.0"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Network().to(device)\n",
        "summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b11x6KpoBz4"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CTCLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, weight_decay=5e-6)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
        "decoder = CTCBeamDecoder(PHONEMES, beam_width = 10, num_processes=os.cpu_count(), log_probs_input=True)# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "### Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "# Use debug = True to see debug outputs\n",
        "def calculate_levenshtein(h, y, lh, ly, decoder, labels, debug = False):\n",
        "\n",
        "    if debug:\n",
        "        print(f\"\\n----- IN LEVENSHTEIN -----\\n\")\n",
        "        # Add any other debug statements as you may need\n",
        "        # you may want to use debug in several places in this function\n",
        "    h = h.permute(1,0,2)    \n",
        "    # TODO: look at docs for CTC.decoder and find out what is returned here\n",
        "    beam_result, beam_scores, timesteps, out_len = decoder.decode(h, seq_lens = lh)\n",
        "\n",
        "    batch_size = y.shape[0] # TODO\n",
        "\n",
        "    distance = 0 # Initialize the distance to be 0 initially\n",
        "\n",
        "    for i in range(batch_size): \n",
        "        h_sliced = beam_result[i][0][:out_len[i][0]] # TODO: Get the output as a sequence of numbers from beam_results\n",
        "        # Remember that h is padded to the max sequence length and lh contains lengths of individual sequences\n",
        "        # Same goes for beam_results and out_lens\n",
        "        # You do not require the padded portion of beam_results - you need to slice it with out_lens \n",
        "        # If it is confusing, print out the shapes of all the variables and try to understand\n",
        "        h_string = \"\".join([str(labels[hh]) for hh in h_sliced])  # TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "\n",
        "        y_sliced = y[i,:ly[i]] # TODO: Do the same for y - slice off the padding with ly\n",
        "        y_string = \"\".join([str(labels[yy])for yy in y_sliced])  # TODO: MAP the sequence of numbers to its corresponding characters with PHONEME_MAP and merge everything as a single string\n",
        "        \n",
        "        distance += Levenshtein.distance(h_string, y_string)\n",
        "\n",
        "\n",
        "    distance /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
        "\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnTLL-5gMBrY",
        "outputId": "742c920e-f8e1-47a7-f284-29c54538b202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 0.20107023417949677\n",
            "torch.Size([1630, 64, 43])\n",
            "lev-distance: 11.984375\n"
          ]
        }
      ],
      "source": [
        "# ANOTEHR SANITY CHECK\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, data in enumerate(train_loader):\n",
        "      \n",
        "      #TODO: \n",
        "      # Follow the following steps, and \n",
        "      # Add some print statements here for sanity checking\n",
        "      \n",
        "      #1. What values are you returning from the collate function\n",
        "      #2. Move the features and target to <DEVICE>\n",
        "      #3. Print the shapes of each to get a fair understanding \n",
        "      #4. Pass the inputs to the model\n",
        "            # Think of the following before you implement:\n",
        "            # 4.1 What will be the input to your model?\n",
        "            # 4.2 What would the model output?\n",
        "            # 4.3 Print the shapes of the output to get a fair understanding \n",
        "\n",
        "      # Calculate loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "      # Calculating the loss is not straightforward. Check the input format of each parameter\n",
        "      x = x.to(device)\n",
        "      h, lh = model(x, lx)\n",
        "      loss = criterion(h,y,lh,ly) # What goes in here?\n",
        "      print(f\"loss: {loss}\")\n",
        "      print(h.size())\n",
        "      distance = calculate_levenshtein(h, y, lh, ly, decoder, PHONEMES)\n",
        "      print(f\"lev-distance: {distance}\")\n",
        "\n",
        "      break # one iteration is enough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlv5pa--i_UO"
      },
      "outputs": [],
      "source": [
        "def train(epoch, model, train_loader, optimizer, criterion):\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=True, position=0, desc=f'Train epoch: {epoch+1}') \n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss, total = 0, 0\n",
        "\n",
        "    log_interval = 40\n",
        "\n",
        "    for batch, (data) in enumerate(train_loader):\n",
        "\n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        total += len(y)\n",
        "\n",
        "        x = x.to(device)\n",
        "        outputs, length = model(x, lx)\n",
        "\n",
        "        loss = criterion(outputs, y, length, ly)\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss = f\"{total_loss/ (batch+1):.4f}\",\n",
        "            lr = f\"{optimizer.param_groups[0]['lr']}\"\n",
        "        )\n",
        "        total_loss += loss.item()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "\n",
        "        batch_bar.update()\n",
        "    batch_bar.close()\n",
        "    return total_loss/(batch+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xq0CtfcUk65Y"
      },
      "outputs": [],
      "source": [
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, leave=True, position=0, desc=f'Validation epoch: {epoch+1}') \n",
        "\n",
        "    total_loss, total_num = 0, 0\n",
        "    total_dist = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (data) in enumerate(val_loader):\n",
        "            x, y, lx, ly = data\n",
        "\n",
        "            x = x.to(device)\n",
        "            outputs, length = model(x, lx)\n",
        "\n",
        "            loss = criterion(outputs, y, length, ly)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            total_dist += calculate_levenshtein(outputs, y, length, ly, decoder, PHONEMES)\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "            batch_bar.set_postfix(\n",
        "                loss = f\"{total_loss/ (batch+1):.4f}\",\n",
        "                lr = f\"{optimizer.param_groups[0]['lr']}\",\n",
        "                distance = f\"{total_dist/ (batch+1):.4f}\"\n",
        "            )\n",
        "            batch_bar.update()\n",
        "    batch_bar.close()\n",
        "    return total_loss/(batch+1), total_dist/len(val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuCiTYLWlLHO",
        "outputId": "49bac4a1-0601-4ce1-dd50-f24f17707979"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train epoch: 1: 100%|██████████| 2072/2072 [44:17<00:00,  1.28s/it, loss=0.1939, lr=5e-05]\n",
            "Validation epoch: 1: 100%|██████████| 43/43 [00:44<00:00,  1.03s/it, distance=7.0673, loss=0.2354, lr=5e-05]\n",
            "Train epoch: 2:  73%|███████▎  | 1517/2072 [32:24<11:52,  1.28s/it, loss=0.1868, lr=5e-05]"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "epochs = 10\n",
        "loss = 5\n",
        "for epoch in range(epochs):\n",
        "    loss_t = train(epoch, model, train_loader, optimizer, criterion)\n",
        "    loss_v, dist_v = validate(model, val_loader, criterion)\n",
        "    scheduler.step(loss_v)\n",
        "    if loss_v < loss:\n",
        "      loss = loss_v\n",
        "      torch.save(model.state_dict(), \"model_best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bq7ioxduBEv"
      },
      "outputs": [],
      "source": [
        "# prediction\n",
        "def predict(test_loader, model, decoder):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        for i,data in enumerate(test_loader):\n",
        "            x,lx=data\n",
        "            x = x.to(device)\n",
        "            h,lh=model(x,lx)\n",
        "\n",
        "            beam_results, beam_scores, timesteps, out_lens = decoder.decode(h.cpu().permute(1,0,2), seq_lens=lh.cpu())\n",
        "            \n",
        "            batch_size = beam_results.shape[0]#What is the batch size\n",
        "\n",
        "            for j in range(batch_size): # Loop through each element in the batch\n",
        "\n",
        "                h_sliced = beam_results[j,0,:out_lens[j][0]]#TODO: Obtain the beam results\n",
        "                h_string = \"\".join([LABELS[x] for x in h_sliced])#TODO: Convert the beam results to phonemes\n",
        "                preds.append(h_string)\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLdjUi6rGnrL",
        "outputId": "43ae51fa-07e2-468e-eec2-2cfcbda10ad7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/model_best.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpSH1uTr2r5f"
      },
      "outputs": [],
      "source": [
        "# decoder_test = CTCBeamDecoder(labels=PHONEMES, beam_width=100, log_probs_input=True)\n",
        "decoder_test = CTCBeamDecoder(labels=LABELS, num_processes=os.cpu_count, beam_width=10, log_probs_input=True)\n",
        "preds = predict(test_loader, model, decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGS1CeDfvr21"
      },
      "outputs": [],
      "source": [
        "print(len(preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBpoc2MA2xsx"
      },
      "outputs": [],
      "source": [
        "with open('submission.csv', 'w') as f:\n",
        "  f.write('index,label\\n')\n",
        "  for i in range(len(preds)):\n",
        "    f.write(str(i) + ',' + preds[i]+\"\\n\")\n",
        "!kaggle competitions submit -c 11-785-f22-hw3p2 -f submission.csv -m \"I made it!\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UR4qfYrVoO4v",
        "gg3-yJ8tok34",
        "j2OqFJkLh70O",
        "hqDrxeHfJw4g",
        "Pt-veYcdL6Fe",
        "NmuPk9J6L8dz"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}